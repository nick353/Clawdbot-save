#!/usr/bin/env python3
"""
RAG Indexing System - ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
"""
import os
import json
import sys
from pathlib import Path
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# è¨­å®š
KNOWLEDGE_DIR = Path("/root/clawd/knowledge")
INDEX_PATH = KNOWLEDGE_DIR / "embeddings.index"
METADATA_PATH = KNOWLEDGE_DIR / "metadata.json"
MODEL_NAME = "all-MiniLM-L6-v2"  # è»½é‡ãƒ»é«˜é€Ÿãƒ¢ãƒ‡ãƒ«

def load_documents():
    """ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ–‡æ›¸ã‚’èª­ã¿è¾¼ã‚€"""
    documents = []
    
    # lessons.md
    lessons_path = Path("/root/clawd/tasks/lessons.md")
    if lessons_path.exists():
        content = lessons_path.read_text()
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã«åˆ†å‰²
        sections = content.split("## ")
        for section in sections[1:]:  # æœ€åˆã®ç©ºã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if section.strip():
                documents.append({
                    "source": "lessons.md",
                    "category": "failure",
                    "content": section.strip()
                })
    
    # successes.md
    successes_path = Path("/root/clawd/tasks/successes.md")
    if successes_path.exists():
        content = successes_path.read_text()
        sections = content.split("## ")
        for section in sections[1:]:
            if section.strip():
                documents.append({
                    "source": "successes.md",
                    "category": "success",
                    "content": section.strip()
                })
    
    # SKILL.md files
    skills_dir = Path("/root/clawd/skills")
    if skills_dir.exists():
        for skill_md in skills_dir.rglob("SKILL.md"):
            content = skill_md.read_text()
            documents.append({
                "source": f"skills/{skill_md.parent.name}/SKILL.md",
                "category": "skill",
                "content": content
            })
    
    return documents

def create_index(documents):
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
    print(f"ðŸ“š {len(documents)} ä»¶ã®æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print(f"ðŸ¤– ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {MODEL_NAME}")
    model = SentenceTransformer(MODEL_NAME)
    
    # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
    print("ðŸ”„ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆä¸­...")
    texts = [doc["content"] for doc in documents]
    embeddings = model.encode(texts, show_progress_bar=True)
    
    # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings.astype('float32'))
    
    # ä¿å­˜
    KNOWLEDGE_DIR.mkdir(exist_ok=True)
    faiss.write_index(index, str(INDEX_PATH))
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        "documents": documents,
        "model": MODEL_NAME,
        "dimension": dimension,
        "count": len(documents)
    }
    with open(METADATA_PATH, "w") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†: {len(documents)} ä»¶")
    print(f"   - Index: {INDEX_PATH}")
    print(f"   - Metadata: {METADATA_PATH}")

def search(query, top_k=3):
    """é¡žä¼¼æ–‡æ›¸ã‚’æ¤œç´¢"""
    if not INDEX_PATH.exists() or not METADATA_PATH.exists():
        print("âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« rag-index.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(METADATA_PATH) as f:
        metadata = json.load(f)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model = SentenceTransformer(metadata["model"])
    
    # ã‚¯ã‚¨ãƒªã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
    query_embedding = model.encode([query])
    
    # æ¤œç´¢
    index = faiss.read_index(str(INDEX_PATH))
    distances, indices = index.search(query_embedding.astype('float32'), top_k)
    
    # çµæžœæ•´å½¢
    results = []
    for i, idx in enumerate(indices[0]):
        doc = metadata["documents"][idx]
        results.append({
            "rank": i + 1,
            "distance": float(distances[0][i]),
            "source": doc["source"],
            "category": doc["category"],
            "content": doc["content"][:500] + "..." if len(doc["content"]) > 500 else doc["content"]
        })
    
    return results

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹:")
        print("  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ: python3 rag-index.py index")
        print("  æ¤œç´¢: python3 rag-index.py search 'ã‚¯ã‚¨ãƒª'")
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command == "index":
        documents = load_documents()
        if not documents:
            print("âš ï¸ æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            sys.exit(0)
        create_index(documents)
    
    elif command == "search":
        if len(sys.argv) < 3:
            print("âŒ ã‚¯ã‚¨ãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(1)
        query = sys.argv[2]
        results = search(query)
        
        print(f"\nðŸ” æ¤œç´¢çµæžœ: '{query}'")
        print("=" * 80)
        for r in results:
            print(f"\nã€{r['rank']}ã€‘ {r['source']} (è·é›¢: {r['distance']:.4f})")
            print(f"ã‚«ãƒ†ã‚´ãƒª: {r['category']}")
            print(f"å†…å®¹:\n{r['content']}\n")
            print("-" * 80)
    
    else:
        print(f"âŒ ä¸æ˜Žãªã‚³ãƒžãƒ³ãƒ‰: {command}")
        sys.exit(1)
